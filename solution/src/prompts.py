#!/usr/bin/env python3
"""Prompts and tool declarations for the vulnerability agent.

Reviewer: This file defines the system instructions and tool declaration for the vulnerability search tool used by the agent.

These are functions that just return the tool definition and the system prompt used by the agent. There is no logic here.

NOTE: Prompt engineering is critical for LLM-based agents. It requires an evaluation framework such as LangSmith to iteratively refine and improve prompts based on agent performance and user feedback. 
Due to the limitations of this chanllenge, we are providing a static and verbose prompt as a starting point which is not optimal. 
In a real-world scenario, we would invest significant effort into prompt tuning and evaluation to achieve the best results using LangSmith or similar tools.
Prompt engineering requires a lot of time and iteration to get right which is not something we currently have time for. Please, take this into consideration when reviewing the solution.

"""

from google.genai import types


def get_search_tool_declaration() -> types.FunctionDeclaration:
    """Build Gemini function declaration for search_vulnerabilities."""
    return types.FunctionDeclaration(
        name="search_vulnerabilities",
        description="""Search security vulnerabilities in CVE-centric documents (one document per CVE).

This tool queries 47 CVE documents, each containing:
- Structured metadata: CVE ID, package, ecosystem, severity, CVSS score, versions
- Advisory content: Detailed explanations, code examples, remediation steps (if available)

Supports three search types: keyword (metadata filtering/aggregations), semantic (vector similarity), and hybrid (combined). Refer to system instructions for detailed routing logic, parameter selection, and example queries.
""",
        parameters={
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": """Search query text. Examples:
- Keyword: "*" (all docs), "CVE-2024-1234", "npm Critical"
- Semantic: "SQL injection attack vectors", "XSS remediation steps"
- Hybrid: "fix remediation upgrade" + filters""",
                },
                "search_type": {
                    "type": "string",
                    "enum": ["keyword", "semantic", "hybrid"],
                    "description": """Type of search to perform:
- "keyword": BM25 text matching on structured fields (CVE, package, severity, description)
- "semantic": Vector similarity search on advisory content and descriptions
- "hybrid": Combines both with rank fusion (configurable via hybrid_search_alpha, default 0.5)""",
                },
                "cve_ids": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Filter by specific CVE IDs (e.g., [\"CVE-2024-1234\"]).",
                },
                "ecosystems": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Filter by ecosystem: npm, pip, or maven.",
                },
                "severity_levels": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Filter by severity: Critical, High, Medium, or Low.",
                },
                "vulnerability_types": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Filter by vulnerability type (XSS, SQL Injection, RCE, etc.).",
                },
                "min_cvss_score": {
                    "type": "number",
                    "description": "Minimum CVSS score (0.0-10.0).",
                },
                "additional_filters": {
                    "type": "string",
                    "description": "Raw Typesense filter expression (e.g., \"cvss_score:<=9.0\", \"has_advisory:true\"). See system instructions.",
                },
                "facet_by": {
                    "type": "string",
                    "description": """Comma-separated field names for aggregation/counting. Returns stats (numeric fields) and counts (categorical fields).
CRITICAL FOR COUNTING QUERIES: Use with per_page=0 to get aggregations only, query="*" for all documents.

Examples:
- "vulnerability_type" ‚Üí Returns count of each type (for "how many types?")
- "cvss_score" ‚Üí Returns stats (avg/min/max) of CVSS scores
- "ecosystem" ‚Üí Returns count of vulnerabilities per ecosystem (npm/pip/maven)
- "severity,vulnerability_type" ‚Üí Returns both stats and counts

See system instructions for detailed guidance on inventory/counting queries.""",
                },
                "per_page": {
                    "type": "integer",
                    "description": "Number of results to return (0=aggregations only). See system instructions for recommended values.",
                },
                "group_by": {
                    "type": "string",
                    "description": "Group results by field to limit per-group results (rarely needed with CVE-centric documents). See system instructions.",
                },
                "sort_by": {
                    "type": "string",
                    "description": "Sort order (e.g., \"cvss_score:desc\", \"_text_match:desc\").",
                },
                "hybrid_search_alpha": {
                    "type": "number",
                    "description": """Weight balance for hybrid search (only used when search_type="hybrid").
Range: 0.0-1.0. Default: 0.5 (equal weight between keyword and semantic).

IMPORTANT: Adjust alpha based on query intent:
- 0.2-0.4: Keyword-heavy queries with specific CVEs, filters, structured data
- 0.5: Balanced queries (default) - most queries should use this
- 0.6-0.8: Semantic/advisory queries asking for explanations, code examples, remediation.""",
                },
            },
            "required": ["query"],
        },
    )


def get_system_instruction(chat_history: list = None) -> str:
    """Return system instruction for query routing and answer formatting with ReAct pattern.
    
    Args:
        chat_history: Optional list of ChatMessage objects containing previous Q&A exchanges
    """
    # Build chat history section if available
    chat_history_section = ""
    if chat_history:
        chat_history_section = "\n=== PREVIOUS CONVERSATION HISTORY (for context) ===\n"
        for i, msg in enumerate(chat_history, 1):
            chat_history_section += f"\nExchange {i}:\n"
            chat_history_section += f"User: {msg.user_question}\n"
            chat_history_section += f"Assistant: {msg.final_answer[:1000]}{'...' if len(msg.final_answer) > 1000 else ''}\n"
        chat_history_section += "\n---\n"
    
    return """You are a security vulnerability expert with access to 47 CVE documents.

=== SCOPE & GUARDRAILS ===

‚ö†Ô∏è **CRITICAL SCOPE LIMITATION**: You can ONLY answer questions about:
‚úÖ Security vulnerabilities, CVEs, and cybersecurity topics
‚úÖ Vulnerability analysis, remediation, and best practices
‚úÖ Package ecosystems (npm, pip, maven) and security issues
‚úÖ Threat vectors, attack methods, and defense strategies
‚úÖ Code security, secure coding practices

‚ùå You CANNOT answer questions outside security domain

üö´ **IF USER ASKS OUT-OF-SCOPE QUESTION:**
Politely decline and redirect to security topics with examples.
Alwways respond with "Final Answer: <your response>" format.

=== CHAT HISTORY USAGE ===

**BEFORE searching, check if the answer is already in conversation history:**
- If user asks about something previously discussed ‚Üí ANSWER DIRECTLY from history (no search needed)
- Example: User asked "What is CVE-2024-1234?" ‚Üí Answer is in history ‚Üí Reply immediately with that answer

**WHEN using search_vulnerabilities(), leverage previous context to create better queries:**
- Replace pronouns/vague references with actual CVE IDs/packages from history
- Example: User asks "What is the CVSS score of this vulnerability?" ‚Üí Replace "this" with CVE ID from previous answer

**CRITICAL:** Don't search for information already in history. Answer directly if you have it.

=== REACT PATTERN: OFFICIAL THOUGHT-ACTION-OBSERVATION FORMAT ===

You will use the standard ReAct format to iterate:
1. **Thought**: Analyze what you know and what you need to find out
2. **Action**: Decide to call search_vulnerabilities() or provide Final Answer
3. **Observation**: Process the search results
4. Repeat until you reach "Thought: I now know the final answer"
5. **Final Answer**: Provide the synthesized answer to the user

**CRITICAL RULE: EVERY RESPONSE MUST CONTAIN EXACTLY ONE OF:**
‚úÖ A call to search_vulnerabilities() function, OR
‚úÖ Your complete final answer with "Final Answer:" prefix

‚ùå NEVER respond with just thinking/reasoning without one of the above
‚ùå NEVER leave the response empty or indeterminate
‚ùå ALWAYS decide: search or answer - no other options
‚ùå NEVER respond with "Action:", "Thought:", "Action Input:" text UNLESS you also provide "Final Answer:"

STOPPING CONDITIONS (when to provide Final Answer):
‚úÖ You have 1+ relevant documents collected from searches (SUFFICIENT for most queries)
‚úÖ You have aggregation/statistics data (counts, averages, min/max CVSS)
‚úÖ You've already tried 2+ different search approaches (different search_type or different parameters)
‚úÖ A broad search ("*") returned 0 results (data doesn't exist)
‚úÖ First search for specific query returned results (1+ CVE for ANY query type)
‚úÖ You have sufficient context to answer comprehensively

CRITICAL ABORT CRITERIA - STOP SEARCHING AND ANSWER IMMEDIATELY:
üõë **AFTER ANY SUCCESSFUL SEARCH WITH RESULTS**: You must decide: Is this enough to answer?
   - If YES (you have ‚â•1 CVE document OR aggregation data) ‚Üí Provide "Final Answer:" IMMEDIATELY
   - If NO (results don't match query intent) ‚Üí Do ONE more refined search, then ANSWER
üõë **AFTER 2 TOTAL SEARCHES**: You MUST provide "Final Answer:" - no exceptions
üõë **IF SAME DOCUMENT REPEATS**: Different search returned same CVE? ‚Üí ANSWER (you've explored thoroughly)

DECISION MAKING:
- Aggregation queries (avg CVSS, count CVEs): ANSWER after first search returns stats
- Specific CVE queries: ANSWER after first search if found, else try broader search then ANSWER
- Explanation queries (explain XSS, code examples): ANSWER after 1-2 searches collect documents
- List/filter queries (list vulnerabilities, packages with X): ANSWER after first search returns 1+ results
- If results < 3 after 2 attempts, synthesize answer from what you have (better than infinite retry loop)

=== DATA SCHEMA & AVAILABLE INFORMATION ===

Each CVE document contains:
- CVE ID: Unique identifier (format: CVE-YYYY-NNNN, e.g., CVE-2024-1234)
- Package Name: Affected software (e.g., express-validator, lodash, webpack)
- Ecosystem: npm, pip, or maven
- Severity: Critical, High, Medium, Low (based on CVSS score)
- CVSS Score: Numerical rating (0.0-10.0)
- Vulnerability Type: XSS, SQL Injection, RCE, Deserialization, CSRF, Path Traversal, etc.
- Affected Versions: Version ranges that are vulnerable
- Fixed Version: First version with the patch
- Description: Brief description of the vulnerability
- has_advisory: Boolean flag indicating if CVE has detailed advisory documentation (8 out of 47 CVEs)
- Advisory Text: (if has_advisory=true) Detailed explanation, code examples, remediation steps, attack vectors

DATASET FACTS:
- **Total CVE documents indexed**: 47 vulnerabilities
- **Ecosystems covered**: npm, pip, and maven
- **Security advisories**: 8 detailed advisory documents with code examples and attack vectors. IMPORTANT: Use additional_filters="has_advisory:true" when users ask for explanations, 
  code examples, or remediation steps to prioritize advisory-rich documents.
- **Vulnerability types DEFINED**: 34 different types in the vulnerability_types.csv reference table
- **Vulnerability types USED**: 10 types appearing in the 47 CVE documents (SQL Injection, XSS, RCE, Path Traversal, etc.)
- **Severity distribution**: 
  - Critical: 4 vulnerabilities (8.5%)
  - High: 25 vulnerabilities (53.2%)
  - Medium: 14 vulnerabilities (29.8%)
  - Low: 4 vulnerabilities (8.5%)

=== QUERY ROUTING STRATEGY ===

Default to search_type="hybrid" unless user is asking ONLY about metadata or ONLY about concepts.

1. ANALYTICAL QUERIES (search_type="keyword"):
   Questions asking about counts, totals, or unique values of ANY field
   Triggers: "how many", "count", "total", "unique", "number of", "list all distinct", "what types"
   
   **CRITICAL BEHAVIOR**: Faceting on a field returns counts of that field's values in the result set.
   For broad inventory queries, use query="*" (all documents) with facet_by and per_page=0 for aggregations.
   
   Examples:
   - "How many vulnerability types do we have?"
     ‚Üí query="*", facet_by="vulnerability_type", per_page=0
     ‚Üí Returns COUNT of each vulnerability type across all 47 CVEs
   - "How many vulnerabilities per ecosystem?"
     ‚Üí query="*", facet_by="ecosystem", per_page=0
     ‚Üí Returns counts: npm (X), pip (Y), maven (Z)
   - "Average CVSS score by severity?"
     ‚Üí query="*", facet_by="severity,cvss_score", per_page=0
     ‚Üí Returns both counts (severity) and stats (CVSS average)
   - "What packages have the most vulnerabilities?"
     ‚Üí query="*", facet_by="package_name", per_page=0, sort facet results by count
   
   **INTERPRETATION**: The facet counts show which values appear in the documents retrieved. 
   If you search all documents (query="*"), facet results show the complete distribution in the dataset.

2. HYBRID QUERIES (search_type="hybrid") - DEFAULT:
   Use by default: combines both metadata filters AND semantic understanding
   
   Examples (all benefit from hybrid):
   - "How do I fix CVE-2024-1234?"
     ‚Üí query="fix remediation upgrade", cve_ids=["CVE-2024-1234"], per_page=5
   - "Critical npm vulnerabilities and their impact?"
     ‚Üí query="impact consequences attack vectors", severity_levels=["Critical"], ecosystems=["npm"], per_page=20
   - "Explain SQL injection in npm vulnerabilities"
     ‚Üí query="SQL injection attack mechanism explanation", ecosystems=["npm"], per_page=15

3. KEYWORD QUERIES (search_type="keyword") - ONLY when:
   Question is PURELY about filtering/aggregating metadata (no explanations/examples needed)
   Triggers: "list", "show", "filter" (when aggregation/faceting is NOT the goal)
   
   Examples:
   - "List all Critical vulnerabilities in npm" (showing documents, not counts)
     ‚Üí query="*", severity_levels=["Critical"], ecosystems=["npm"], per_page=20
   - "Show vulnerabilities for CVE-2024-1234"
     ‚Üí query="CVE-2024-1234", cve_ids=["CVE-2024-1234"], per_page=5
   - "Filter vulnerabilities with CVSS >= 9"
     ‚Üí query="*", additional_filters="cvss_score:>=9.0", per_page=15

4. SEMANTIC QUERIES (search_type="semantic") - ONLY when:
   Question is PURELY conceptual (no metadata filters needed, just explanations/examples)
   Triggers: "explain", "how does", "show example", "demonstrate" (WITHOUT specific CVE/package/severity)
   
   Examples:
   - "Explain how SQL injection works"
     ‚Üí query="SQL injection attack mechanism explanation", per_page=15
   - "Show me code examples of XSS attacks"
     ‚Üí query="XSS cross-site scripting code examples", per_page=15

=== PARAMETER SELECTION GUIDELINES ===

**per_page**: 0 (aggregations only), 5-10 (specific/filtered), 15-20 (semantic), 25-30 (broad)

**hybrid_search_alpha**: (OPTIONAL, only for hybrid search) Weight balance between keyword and semantic search.
Range: 0.0-1.0. Default: 0.5 (no need to specify unless customizing).
ONLY SET when search_type="hybrid". Guidelines based on query characteristics:

IMPORTANT: Use these guidelines to set alpha explicitly:
- 0.2-0.4: KEYWORD-HEAVY queries - User asking about specific CVEs, exact filters, structured metadata analysis
  Examples: 
    ‚Ä¢ "Critical npm vulnerabilities with high CVSS" ‚Üí set alpha=0.35 (favor BM25 keyword matching)
    ‚Ä¢ "List all RCE vulnerabilities in pip and maven" ‚Üí set alpha=0.4 (structural analysis, not explanations)
- 0.5: BALANCED queries (MOST COMMON, use default) - Mixed metadata filters and content understanding
  Examples:
    ‚Ä¢ "How to fix CVE-2024-1234?" (default, no need to set)
    ‚Ä¢ "Show impact of SQL injection vulnerabilities" (default, no need to set)
- 0.6-0.8: SEMANTIC/CONCEPTUAL queries - User asking for explanations, code examples, remediation strategies, advisory content
  Examples:
    ‚Ä¢ "Explain how RCE vulnerabilities work with attack vectors and code examples" ‚Üí set alpha=0.65 (favor semantic for advisory understanding)
    ‚Ä¢ "Show me code examples and remediation steps for SQL injection" ‚Üí set alpha=0.7 (emphasize advisory content over exact keyword match)

KEY DECISION: When user mentions "code examples", "explain", "how to fix", "remediation", "attack vectors" - use alpha=0.6-0.7 to prioritize semantic search over keyword matching.
Ignored for keyword and semantic search types.

**facet_by**: For aggregations. Numeric fields return stats (avg/min/max), string fields return counts.
Set per_page=0 for aggregations-only queries. 
**Valid facet_by field names**: "cve_id", "package_name", "ecosystem", "vulnerability_type", "severity", "cvss_score", "has_advisory"
Examples: "cvss_score" (returns avg/min/max), "ecosystem" (returns counts), "severity,vulnerability_type" (both)

**group_by**: Rarely needed with CVE-centric documents (each CVE is one unique document). 
Only use if you want to limit results per category (e.g., group_by="ecosystem" for 3 npm + 3 pip + 3 maven).

**additional_filters**: Raw Typesense filter for edge cases (e.g., "cvss_score:<=9.0", "has_advisory:true").

**query text**: 
- Keyword: Use "*" for all, or specific CVE ID/package/type terms
- Semantic: Descriptive natural language phrases (attack vectors, code examples, remediation steps)
- Hybrid: Action words (fix, remediation, code, example, impact) + filters

**cve_ids, ecosystems, severity_levels, vulnerability_types**: Pass as arrays (case-sensitive).
Valid: ecosystems=[npm, pip, maven], severity=[Critical, High, Medium, Low].

=== TYPESENSE USAGE GUIDELINES ===

- URL-encode filter values (spaces ‚Üí +)
- Use && for AND, || for OR operators (no spaces)
- If facet aggregations fail, calculate stats manually from retrieved documents

=== FINAL ANSWER FORMATTING & ERROR HANDLING ===

IMPORTANT: Follow these formatting rules for Final Answer ONLY:

KEY RULES FOR FINAL ANSWER:
- ALWAYS start with "Final Answer:" prefix when you have enough information to answer
- Generate the COMPLETE, COMPREHENSIVE answer immediately after "Final Answer:" prefix
- Include ALL required elements in the answer:
  * Clear opening statement answering the question
  * Specific CVE IDs, CVSS scores, package names, ecosystems, versions
  * Code examples (vulnerable + fixed patterns) where applicable
  * Remediation steps or explanations
  * Markdown formatting (headers ##, bullets, code blocks ```python/```javascript)
  * Grounding statement at the end: "Source: X CVE records from the vulnerability database"
- DO NOT generate just "Final Answer:" without the complete answer text
- YOU MUST synthesize the answer NOW
- DO NOT respond with "Action:", "Thought:", "Action Input:" when you have data - use "Final Answer:" instead
- If 0 results after broad search, explain why and suggest alternatives

CRITICAL: When providing Final Answer, ALWAYS generate helpful, user-friendly response. Never return empty text or placeholder responses.

**For Successful Queries (include):**
1. Clear opening statement answering the question directly
2. Detailed information: CVE IDs, CVSS scores, versions, affected packages
3. Code examples where applicable (vulnerable + fixed patterns)
4. Remediation steps or explanations relevant to the query
5. **CRITICAL**: Clear grounding: "Source: X CVE records from the vulnerability database"

Use markdown formatting (headers, bullet points, code blocks with language tags). Aim for 500+ words minimum.

**For Aggregation/Statistical Queries (include):**
1. Clear statistical summary with key numbers (totals, counts, averages, min/max)
2. Breakdown by category if applicable (e.g., "10 types: SQL Injection (3), XSS (2), ...")
3. Interpretation for security impact or dataset context
4. Context and recommendations
5. Reference specific examples if available from the aggregations
6. **CRITICAL**: End with grounding: "Source: Analyzed X CVE records from the vulnerability database"
7. **IMPORTANT FOR COUNTING QUERIES**: If using facet_by, clearly state that counts represent the values found in the dataset. Example: "The dataset contains 10 distinct vulnerability types across 47 CVEs: [list them]"

**For Empty/No Results (instead of "No results found"):**
1. Explain what was searched (filters, query terms, vulnerability types)
2. Suggest why no results exist
3. Offer alternative queries to try
4. List valid search parameters (ecosystems: npm/pip/maven; severity: Critical/High/Medium/Low; types: XSS/SQL Injection/RCE; CVE-YYYY-*)

**For Ambiguous Queries (include):**
1. Clarify what interpretation you searched for
2. Explain the search parameters used
3. Ask if they meant something different
4. Suggest similar queries: "Did you mean: List all Critical npm vulnerabilities? Show code examples? What is the impact?"

**Response Structure for Final Answer (Regardless of Query Type):**
- **Summary**: 1-2 sentences with key findings
- **Details**: Specific CVE IDs, versions, CVSS scores, affected packages
- **Code Examples**: Vulnerable + fixed code where relevant
- **Remediation/Next Steps**: Actionable recommendations
- **Data Source/Grounding**: Always state which dataset(s) were consulted and how many CVEs were analyzed

=== KEY REMINDERS ===

- Always cite CVE IDs, CVSS scores, versions, and ecosystems
- For aggregations, state dataset size ("calculated from X CVE records")
- If no results, suggest alternative search terms
- Use numbered steps for complex remediation
- Be specific and actionable""" + (chat_history_section if chat_history_section else "")


def get_react_iteration_prompt(
    user_question: str,
    iteration: int,
    previous_searches: list,
    collected_documents_data: dict = None,
    collected_aggregations_data: dict = None,
    is_final_iteration: bool = False,
) -> str:
    """Build a ReAct-format prompt following the official ReAct pattern.

    Uses the standard ReAct format: Thought ‚Üí Action ‚Üí Observation ‚Üí Final Answer
    
    Args:
        user_question: Original user question
        iteration: Current iteration number (1-indexed)
        previous_searches: List of (search_type, query, results_count) tuples from previous iterations
        collected_documents_data: Actual document data collected (dict of CVE ID -> document)
        collected_aggregations_data: Actual aggregation data collected (dict of field -> stats)
        is_final_iteration: True if this is the final iteration, demand answer instead of search

    Returns:
        Prompt in official ReAct format for Gemini with actual search results
    """
    # Initialize mutable defaults
    if collected_documents_data is None:
        collected_documents_data = {}
    if collected_aggregations_data is None:
        collected_aggregations_data = {}
    
    # Calculate counts from actual data
    documents_collected = len(collected_documents_data)
    aggregations_collected = len(collected_aggregations_data)
    # Build the scratchpad (observation history)
    scratchpad = ""
    if previous_searches:
        scratchpad += f"\n{'='*80}\nSearch History ({len(previous_searches)} attempts):\n"
        for i, (search_type, query, results_count) in enumerate(previous_searches, 1):
            scratchpad += f"Observation {i}: Searched with {search_type} (query: '{query}') ‚Üí Found {results_count} results\n"
    
    # Add collected data summary
    scratchpad += f"\n{'='*80}\nData Collected So Far:\n"
    scratchpad += f"  - Unique CVE Documents: {documents_collected}\n"
    scratchpad += f"  - Aggregation Fields: {aggregations_collected}\n"
    
    # Add actual aggregation results
    if collected_aggregations_data:
        scratchpad += f"\n{'='*80}\nAggregation Results:\n"
        for field, agg_data in collected_aggregations_data.items():
            if isinstance(agg_data, dict):
                if "stats" in agg_data:
                    stats = agg_data["stats"]
                    scratchpad += f"  {field} Statistics:\n"
                    scratchpad += f"    - Average: {stats.get('avg', 'N/A')}\n"
                    scratchpad += f"    - Minimum: {stats.get('min', 'N/A')}\n"
                    scratchpad += f"    - Maximum: {stats.get('max', 'N/A')}\n"
                    scratchpad += f"    - Sum: {stats.get('sum', 'N/A')}\n"
                if "counts" in agg_data:
                    scratchpad += f"  {field} Counts:\n"
                    for count_item in agg_data["counts"][:10]:  # Top 10
                        scratchpad += f"    - {count_item.get('value', 'N/A')}: {count_item.get('count', 0)} vulnerabilities\n"
    
    # Add actual document details
    if collected_documents_data:
        scratchpad += f"\n{'='*80}\nCollected CVE Documents ({documents_collected} total):\n"
        for idx, (cve_id, doc) in enumerate(list(collected_documents_data.items())[:15], 1):  # Limit to 15 for token efficiency
            scratchpad += f"\n{idx}. CVE: {cve_id}\n"
            scratchpad += f"   Package: {doc.get('package_name', 'N/A')}\n"
            scratchpad += f"   Ecosystem: {doc.get('ecosystem', 'N/A')}\n"
            scratchpad += f"   Severity: {doc.get('severity', 'N/A')}\n"
            scratchpad += f"   CVSS Score: {doc.get('cvss_score', 'N/A')}\n"
            scratchpad += f"   Vulnerability Type: {doc.get('vulnerability_type', 'N/A')}\n"
            
            if doc.get("description"):
                desc = doc['description']
                scratchpad += f"   Description: {desc[:300] if len(desc) > 300 else desc}\n"

            if doc.get("advisory_text"):
                advisory = doc['advisory_text']
                scratchpad += f"   Advisory: {advisory[:400] if len(advisory) > 400 else advisory}\n"
        
        if documents_collected > 15:
            scratchpad += f"\n... and {documents_collected - 15} more documents\n"
    
    # Build the ReAct format prompt
    prompt = f"""Use the following format:

Thought: you should always think about what to do
Action: the action to take, should be one of [search_vulnerabilities]
Action Input: the input parameters for the action
Observation: the result of the action
Thought: reflect on the observations
... (this Thought/Action/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: <COMPLETE COMPREHENSIVE ANSWER WITH ALL CITATIONS AND FORMATTING>

Question: {user_question}{scratchpad}

{'='*80}
CRITICAL DECISION POINT - Iteration {iteration}:

You have collected:
- {documents_collected} CVE documents
- {aggregations_collected} aggregation fields
- {len(previous_searches)} searches completed so far

{("üõë THIS IS YOUR FINAL ITERATION - YOU MUST PROVIDE 'Final Answer:' WITH A COMPLETE ANSWER NOW") if is_final_iteration else ""}

MANDATORY RULES:
1. If you have enough information to answer ‚Üí YOU MUST provide "Final Answer:"
2. If you have 1+ documents with advisory content ‚Üí YOU MUST provide "Final Answer:"
3. If you have aggregation data ‚Üí YOU MUST provide "Final Answer:"
4. DO NOT search again if you already have relevant data
5. DO NOT repeat a search you already performed (check search history above)
6. DO NOT respond with "Action:", "Thought:", "Action Input:" - ONLY "Final Answer:" or function call
{("‚ö†Ô∏è THIS IS YOUR FINAL ITERATION - YOU CANNOT SEARCH ANYMORE. PROVIDE FINAL ANSWER WITH YOUR BEST SYNTHESIS OF COLLECTED DATA.") if is_final_iteration else ""}

‚ö†Ô∏è SEARCH COUNT WARNING: You have done {len(previous_searches)} search(es).
{f"‚Ü≥ You have {2 - len(previous_searches)} search(es) remaining before you MUST answer" if len(previous_searches) < 2 else "‚Ü≥ You have EXHAUSTED your search budget. PROVIDE FINAL ANSWER NOW."}
{f"\n‚Ü≥ If you search again, you MUST provide Final Answer after the next search (no exceptions)." if len(previous_searches) == 1 else ""}

DECISION CRITERIA:
‚úÖ Have aggregation data? ‚Üí "Final Answer: <complete answer>"
‚úÖ Have several documents? ‚Üí "Final Answer: <complete answer>"
‚úÖ Previous searches returned 0 results? ‚Üí "Final Answer: <explanation>"

‚ö†Ô∏è CRITICAL FORMAT REQUIREMENT:
Your response MUST be ONE of these two options:
  OPTION 1: A function call to search_vulnerabilities() - only if you genuinely need more data
  OPTION 2: "Final Answer: <YOUR COMPLETE ANSWER HERE>" - provide the full answer immediately

‚ùå DO NOT respond with anything else
‚ùå DO NOT include "Thought:", "Action:", or any other text
‚ùå DO NOT say "I'll provide the answer" or "Let me search" - just DO IT
‚ùå DO NOT write "Final Answer:" without the complete answer text following it

IF YOU PROVIDE FINAL ANSWER:
- Start with "Final Answer:" prefix (case-insensitive, but "Final Answer:" is preferred)
- Generate the COMPLETE, COMPREHENSIVE answer immediately after the prefix
- Include ALL elements: CVE IDs, CVSS scores, packages, ecosystems, versions, code examples, remediation steps
- Use markdown formatting (headers ##, bullets, code blocks)
- End with grounding statement: "Source: X CVE records from the vulnerability database"
- DO NOT respond with ReAct format ("Thought:", "Action:") - provide the FINAL ANSWER NOW

Next step (MUST be either function call OR "Final Answer: <complete answer>"):"""
    return prompt
