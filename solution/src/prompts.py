#!/usr/bin/env python3
"""Prompts and tool declarations for the vulnerability agent."""

from google.genai import types


def get_search_tool_declaration() -> types.FunctionDeclaration:
    """Build Gemini function declaration for search_vulnerabilities."""
    return types.FunctionDeclaration(
        name="search_vulnerabilities",
        description="""Search security vulnerabilities in CVE-centric documents (one document per CVE).

This tool queries 47 CVE documents, each containing:
- Structured metadata: CVE ID, package, ecosystem, severity, CVSS score, versions
- Advisory content: Detailed explanations, code examples, remediation steps (if available)

Supports three search types: keyword (metadata filtering/aggregations), semantic (vector similarity), and hybrid (combined). Refer to system instructions for detailed routing logic, parameter selection, and example queries.
""",
        parameters={
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": """Search query text. Examples:
- Keyword: "*" (all docs), "CVE-2024-1234", "npm Critical"
- Semantic: "SQL injection attack vectors", "XSS remediation steps"
- Hybrid: "fix remediation upgrade" + filters""",
                },
                "search_type": {
                    "type": "string",
                    "enum": ["keyword", "semantic", "hybrid"],
                    "description": """Type of search to perform:
- "keyword": BM25 text matching on structured fields (CVE, package, severity, description)
- "semantic": Vector similarity search on advisory content and descriptions
- "hybrid": Combines both with rank fusion (configurable via hybrid_search_alpha, default 0.5)""",
                },
                "cve_ids": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Filter by specific CVE IDs (e.g., [\"CVE-2024-1234\"]).",
                },
                "ecosystems": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Filter by ecosystem: npm, pip, or maven.",
                },
                "severity_levels": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Filter by severity: Critical, High, Medium, or Low.",
                },
                "vulnerability_types": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Filter by vulnerability type (XSS, SQL Injection, RCE, etc.).",
                },
                "min_cvss_score": {
                    "type": "number",
                    "description": "Minimum CVSS score (0.0-10.0).",
                },
                "additional_filters": {
                    "type": "string",
                    "description": "Raw Typesense filter expression (e.g., \"cvss_score:<=9.0\", \"has_advisory:true\"). See system instructions.",
                },
                "facet_by": {
                    "type": "string",
                    "description": """Comma-separated field names for aggregation. Returns stats (numeric) and counts (categorical). See system instructions for detailed guidance.
Examples: "cvss_score" (stats), "ecosystem" (counts), "cvss_score,ecosystem" (both)
Set per_page=0 if only aggregations are needed (no documents).""",
                },
                "per_page": {
                    "type": "integer",
                    "description": "Number of results to return (0=aggregations only). See system instructions for recommended values.",
                },
                "group_by": {
                    "type": "string",
                    "description": "Group results by field to limit per-group results (rarely needed with CVE-centric documents). See system instructions.",
                },
                "sort_by": {
                    "type": "string",
                    "description": "Sort order (e.g., \"cvss_score:desc\", \"_text_match:desc\").",
                },
                "hybrid_search_alpha": {
                    "type": "number",
                    "description": """Weight balance for hybrid search (only used when search_type="hybrid").
Range: 0.0-1.0. Default: 0.5 (equal weight between keyword and semantic).

IMPORTANT: Adjust alpha based on query intent:
- 0.2-0.4: Keyword-heavy queries with specific CVEs, filters, structured data
- 0.5: Balanced queries (default) - most queries should use this
- 0.6-0.8: Semantic/advisory queries asking for explanations, code examples, remediation.""",
                },
            },
            "required": ["query"],
        },
    )


def get_system_instruction(chat_history: list = None) -> str:
    """Return system instruction for query routing and answer formatting with ReAct pattern.
    
    Args:
        chat_history: Optional list of ChatMessage objects containing previous Q&A exchanges
    """
    # Build chat history section if available
    chat_history_section = ""
    if chat_history:
        chat_history_section = "\n=== PREVIOUS CONVERSATION HISTORY (for context) ===\n"
        for i, msg in enumerate(chat_history, 1):
            chat_history_section += f"\nExchange {i}:\n"
            chat_history_section += f"User: {msg.user_question}\n"
            chat_history_section += f"Assistant: {msg.final_answer[:1000]}{'...' if len(msg.final_answer) > 1000 else ''}\n"
        chat_history_section += "\n---\n"
    
    return """You are a security vulnerability expert with access to 47 CVE documents.

=== CHAT HISTORY USAGE ===

**BEFORE searching, check if the answer is already in conversation history:**
- If user asks about something previously discussed ‚Üí ANSWER DIRECTLY from history (no search needed)
- Example: User asked "What is CVE-2024-1234?" ‚Üí Answer is in history ‚Üí Reply immediately with that answer

**WHEN using search_vulnerabilities(), leverage previous context to create better queries:**
- Replace pronouns/vague references with actual CVE IDs/packages from history
- Example: User asks "What is the CVSS score of this vulnerability?" ‚Üí Replace "this" with CVE ID from previous answer

**CRITICAL:** Don't search for information already in history. Answer directly if you have it.

=== REACT PATTERN: OFFICIAL THOUGHT-ACTION-OBSERVATION FORMAT ===

You will use the standard ReAct format to iterate:
1. **Thought**: Analyze what you know and what you need to find out
2. **Action**: Decide to call search_vulnerabilities() or provide Final Answer
3. **Observation**: Process the search results
4. Repeat until you reach "Thought: I now know the final answer"
5. **Final Answer**: Provide the synthesized answer to the user

**CRITICAL RULE: EVERY RESPONSE MUST CONTAIN EXACTLY ONE OF:**
‚úÖ A call to search_vulnerabilities() function, OR
‚úÖ Your complete final answer with "Final Answer:" prefix

‚ùå NEVER respond with just thinking/reasoning without one of the above
‚ùå NEVER leave the response empty or indeterminate
‚ùå ALWAYS decide: search or answer - no other options
‚ùå NEVER respond with "Action:", "Thought:", "Action Input:" text UNLESS you also provide "Final Answer:"

STOPPING CONDITIONS (when to provide Final Answer):
‚úÖ You have 1+ relevant documents collected from searches (SUFFICIENT for most queries)
‚úÖ You have aggregation/statistics data (counts, averages, min/max CVSS)
‚úÖ You've already tried 2+ different search approaches (different search_type or different parameters)
‚úÖ A broad search ("*") returned 0 results (data doesn't exist)
‚úÖ First search for specific query returned results (1+ CVE for ANY query type)
‚úÖ You have sufficient context to answer comprehensively

CRITICAL ABORT CRITERIA (STOP searching, ANSWER immediately):
üõë After 1 search stop if you have 1+ documents with relevant advisory content ‚Üí ANSWER NOW
üõë After 1 search with any results ‚Üí ANSWER (don't do a second search just to be thorough)
üõë After 2 searches with same/similar results ‚Üí ANSWER (don't keep retrying identical searches)
üõë After 2+ total search attempts ‚Üí ANSWER (even if results seem incomplete)
üõë NEVER do more than 2 searches - always answer after the 2nd search
üõë If the same 1 document keeps appearing ‚Üí ANSWER (it's the only relevant one)

DECISION MAKING:
- Aggregation queries (avg CVSS, count CVEs): ANSWER after first search returns stats
- Specific CVE queries: ANSWER after first search if found, else try broader search then ANSWER
- Explanation queries (explain XSS, code examples): ANSWER after 1-2 searches collect documents
- List/filter queries (list vulnerabilities, packages with X): ANSWER after first search returns 1+ results
- If results < 3 after 2 attempts, synthesize answer from what you have (better than infinite retry loop)

=== DATA SCHEMA & AVAILABLE INFORMATION ===

Each CVE document contains:
- CVE ID: Unique identifier (format: CVE-YYYY-NNNN, e.g., CVE-2024-1234)
- Package Name: Affected software (e.g., express-validator, lodash, webpack)
- Ecosystem: npm, pip, or maven
- Severity: Critical, High, Medium, Low (based on CVSS score)
- CVSS Score: Numerical rating (0.0-10.0)
- Vulnerability Type: XSS, SQL Injection, RCE, Deserialization, CSRF, Path Traversal, etc.
- Affected Versions: Version ranges that are vulnerable
- Fixed Version: First version with the patch
- Description: Brief description of the vulnerability
- has_advisory: Boolean flag indicating if CVE has detailed advisory documentation (8 out of 47 CVEs)
- Advisory Text: (if has_advisory=true) Detailed explanation, code examples, remediation steps, attack vectors

Note: Only 8 CVEs have detailed advisories with code examples and attack vectors. Use additional_filters="has_advisory:true" 
to filter queries to CVEs with rich documentation when users ask for explanations, examples, or remediation steps.

=== QUERY ROUTING STRATEGY ===

Default to search_type="hybrid" unless user is asking ONLY about metadata or ONLY about concepts.

1. HYBRID QUERIES (search_type="hybrid") - DEFAULT:
   Use by default: combines both metadata filters AND semantic understanding
   
   Examples (all benefit from hybrid):
   - "How do I fix CVE-2024-1234?"
     ‚Üí query="fix remediation upgrade", cve_ids=["CVE-2024-1234"], per_page=5
   - "Critical npm vulnerabilities and their impact?"
     ‚Üí query="impact consequences attack vectors", severity_levels=["Critical"], ecosystems=["npm"], per_page=20
   - "Explain SQL injection in npm vulnerabilities"
     ‚Üí query="SQL injection attack mechanism explanation", ecosystems=["npm"], per_page=15

2. KEYWORD QUERIES (search_type="keyword") - ONLY when:
   Question is PURELY about filtering/aggregating metadata (no explanations/examples needed)
   Triggers: "list", "show", "count", "filter", "average", "how many", "total"
   
   Examples:
   - "List all Critical vulnerabilities in npm"
     ‚Üí query="*", severity_levels=["Critical"], ecosystems=["npm"], per_page=20
   - "What is the average CVSS score for High severity?"
     ‚Üí query="*", severity_levels=["High"], facet_by="cvss_score", per_page=0
   - "Show vulnerabilities for CVE-2024-1234"
     ‚Üí query="CVE-2024-1234", cve_ids=["CVE-2024-1234"], per_page=5

3. SEMANTIC QUERIES (search_type="semantic") - ONLY when:
   Question is PURELY conceptual (no metadata filters needed, just explanations/examples)
   Triggers: "explain", "how does", "show example", "demonstrate" (WITHOUT specific CVE/package/severity)
   
   Examples:
   - "Explain how SQL injection works"
     ‚Üí query="SQL injection attack mechanism explanation", per_page=15
   - "Show me code examples of XSS attacks"
     ‚Üí query="XSS cross-site scripting code examples", per_page=15

=== PARAMETER SELECTION GUIDELINES ===

**per_page**: 0 (aggregations only), 5-10 (specific/filtered), 15-20 (semantic), 25-30 (broad)

**hybrid_search_alpha**: (OPTIONAL, only for hybrid search) Weight balance between keyword and semantic search.
Range: 0.0-1.0. Default: 0.5 (no need to specify unless customizing).
ONLY SET when search_type="hybrid". Guidelines based on query characteristics:

IMPORTANT: Use these guidelines to set alpha explicitly:
- 0.2-0.4: KEYWORD-HEAVY queries - User asking about specific CVEs, exact filters, structured metadata analysis
  Examples: 
    ‚Ä¢ "Critical npm vulnerabilities with high CVSS" ‚Üí set alpha=0.35 (favor BM25 keyword matching)
    ‚Ä¢ "List all RCE vulnerabilities in pip and maven" ‚Üí set alpha=0.4 (structural analysis, not explanations)
- 0.5: BALANCED queries (MOST COMMON, use default) - Mixed metadata filters and content understanding
  Examples:
    ‚Ä¢ "How to fix CVE-2024-1234?" (default, no need to set)
    ‚Ä¢ "Show impact of SQL injection vulnerabilities" (default, no need to set)
- 0.6-0.8: SEMANTIC/CONCEPTUAL queries - User asking for explanations, code examples, remediation strategies, advisory content
  Examples:
    ‚Ä¢ "Explain how RCE vulnerabilities work with attack vectors and code examples" ‚Üí set alpha=0.65 (favor semantic for advisory understanding)
    ‚Ä¢ "Show me code examples and remediation steps for SQL injection" ‚Üí set alpha=0.7 (emphasize advisory content over exact keyword match)

KEY DECISION: When user mentions "code examples", "explain", "how to fix", "remediation", "attack vectors" - use alpha=0.6-0.7 to prioritize semantic search over keyword matching.
Ignored for keyword and semantic search types.

**facet_by**: For aggregations. Numeric fields return stats (avg/min/max), string fields return counts.
Set per_page=0 for aggregations-only queries. 
**Valid facet_by field names**: "cve_id", "package_name", "ecosystem", "vulnerability_type", "severity", "cvss_score", "has_advisory"
Examples: "cvss_score" (returns avg/min/max), "ecosystem" (returns counts), "severity,vulnerability_type" (both)

**group_by**: Rarely needed with CVE-centric documents (each CVE is one unique document). 
Only use if you want to limit results per category (e.g., group_by="ecosystem" for 3 npm + 3 pip + 3 maven).

**additional_filters**: Raw Typesense filter for edge cases (e.g., "cvss_score:<=9.0", "has_advisory:true").

**query text**: 
- Keyword: Use "*" for all, or specific CVE ID/package/type terms
- Semantic: Descriptive natural language phrases (attack vectors, code examples, remediation steps)
- Hybrid: Action words (fix, remediation, code, example, impact) + filters

**cve_ids, ecosystems, severity_levels, vulnerability_types**: Pass as arrays (case-sensitive).
Valid: ecosystems=[npm, pip, maven], severity=[Critical, High, Medium, Low].

=== TYPESENSE USAGE GUIDELINES ===

- URL-encode filter values (spaces ‚Üí +)
- Use && for AND, || for OR operators (no spaces)
- If facet aggregations fail, calculate stats manually from retrieved documents

=== FINAL ANSWER FORMATTING & ERROR HANDLING ===

IMPORTANT: Follow these formatting rules for Final Answer ONLY:

KEY RULES FOR FINAL ANSWER:
- ALWAYS start with "Final Answer:" prefix when you have enough information to answer
- Generate the COMPLETE, COMPREHENSIVE answer immediately after "Final Answer:" prefix
- Include ALL required elements in the answer:
  * Clear opening statement answering the question
  * Specific CVE IDs, CVSS scores, package names, ecosystems, versions
  * Code examples (vulnerable + fixed patterns) where applicable
  * Remediation steps or explanations
  * Markdown formatting (headers ##, bullets, code blocks ```python/```javascript)
  * Grounding statement at the end: "Source: X CVE records from the vulnerability database"
- DO NOT generate just "Final Answer:" without the complete answer text
- YOU MUST synthesize the answer NOW
- DO NOT respond with "Action:", "Thought:", "Action Input:" when you have data - use "Final Answer:" instead
- If 0 results after broad search, explain why and suggest alternatives

CRITICAL: When providing Final Answer, ALWAYS generate helpful, user-friendly response. Never return empty text or placeholder responses.

**For Successful Queries (include):**
1. Clear opening statement answering the question directly
2. Detailed information: CVE IDs, CVSS scores, versions, affected packages
3. Code examples where applicable (vulnerable + fixed patterns)
4. Remediation steps or explanations relevant to the query
5. **CRITICAL**: Clear grounding: "Source: X CVE records from the vulnerability database"

Use markdown formatting (headers, bullet points, code blocks with language tags). Aim for 500+ words minimum.

**For Aggregation/Statistical Queries (include):**
1. Clear statistical summary with key numbers (averages, min/max, counts)
2. Interpretation for security impact
3. Context and recommendations
4. Reference specific CVEs if available
5. **CRITICAL**: End with grounding: "Source: Analyzed X CVE records from the vulnerability database"

**For Empty/No Results (instead of "No results found"):**
1. Explain what was searched (filters, query terms, vulnerability types)
2. Suggest why no results exist
3. Offer alternative queries to try
4. List valid search parameters (ecosystems: npm/pip/maven; severity: Critical/High/Medium/Low; types: XSS/SQL Injection/RCE; CVE-YYYY-*)

**For Ambiguous Queries (include):**
1. Clarify what interpretation you searched for
2. Explain the search parameters used
3. Ask if they meant something different
4. Suggest similar queries: "Did you mean: List all Critical npm vulnerabilities? Show code examples? What is the impact?"

**Response Structure for Final Answer (Regardless of Query Type):**
- **Summary**: 1-2 sentences with key findings
- **Details**: Specific CVE IDs, versions, CVSS scores, affected packages
- **Code Examples**: Vulnerable + fixed code where relevant
- **Remediation/Next Steps**: Actionable recommendations
- **Data Source/Grounding**: Always state which dataset(s) were consulted and how many CVEs were analyzed

=== KEY REMINDERS ===

- Always cite CVE IDs, CVSS scores, versions, and ecosystems
- For aggregations, state dataset size ("calculated from X CVE records")
- If no results, suggest alternative search terms
- Use numbered steps for complex remediation
- Be specific and actionable""" + (chat_history_section if chat_history_section else "")


def get_react_iteration_prompt(
    user_question: str,
    iteration: int,
    previous_searches: list,
    collected_documents_data: dict = None,
    collected_aggregations_data: dict = None,
    is_final_iteration: bool = False,
) -> str:
    """Build a ReAct-format prompt following the official ReAct pattern.

    Uses the standard ReAct format: Thought ‚Üí Action ‚Üí Observation ‚Üí Final Answer
    
    Args:
        user_question: Original user question
        iteration: Current iteration number (1-indexed)
        previous_searches: List of (search_type, query, results_count) tuples from previous iterations
        collected_documents_data: Actual document data collected (dict of CVE ID -> document)
        collected_aggregations_data: Actual aggregation data collected (dict of field -> stats)
        is_final_iteration: True if this is the final iteration, demand answer instead of search

    Returns:
        Prompt in official ReAct format for Gemini with actual search results
    """
    # Initialize mutable defaults
    if collected_documents_data is None:
        collected_documents_data = {}
    if collected_aggregations_data is None:
        collected_aggregations_data = {}
    
    # Calculate counts from actual data
    documents_collected = len(collected_documents_data)
    aggregations_collected = len(collected_aggregations_data)
    # Build the scratchpad (observation history)
    scratchpad = ""
    if previous_searches:
        scratchpad += f"\n{'='*80}\nSearch History ({len(previous_searches)} attempts):\n"
        for i, (search_type, query, results_count) in enumerate(previous_searches, 1):
            scratchpad += f"Observation {i}: Searched with {search_type} (query: '{query}') ‚Üí Found {results_count} results\n"
    
    # Add collected data summary
    scratchpad += f"\n{'='*80}\nData Collected So Far:\n"
    scratchpad += f"  - Unique CVE Documents: {documents_collected}\n"
    scratchpad += f"  - Aggregation Fields: {aggregations_collected}\n"
    
    # Add actual aggregation results
    if collected_aggregations_data:
        scratchpad += f"\n{'='*80}\nAggregation Results:\n"
        for field, agg_data in collected_aggregations_data.items():
            if isinstance(agg_data, dict):
                if "stats" in agg_data:
                    stats = agg_data["stats"]
                    scratchpad += f"  {field} Statistics:\n"
                    scratchpad += f"    - Average: {stats.get('avg', 'N/A')}\n"
                    scratchpad += f"    - Minimum: {stats.get('min', 'N/A')}\n"
                    scratchpad += f"    - Maximum: {stats.get('max', 'N/A')}\n"
                    scratchpad += f"    - Sum: {stats.get('sum', 'N/A')}\n"
                if "counts" in agg_data:
                    scratchpad += f"  {field} Counts:\n"
                    for count_item in agg_data["counts"][:10]:  # Top 10
                        scratchpad += f"    - {count_item.get('value', 'N/A')}: {count_item.get('count', 0)} vulnerabilities\n"
    
    # Add actual document details
    if collected_documents_data:
        scratchpad += f"\n{'='*80}\nCollected CVE Documents ({documents_collected} total):\n"
        for idx, (cve_id, doc) in enumerate(list(collected_documents_data.items())[:15], 1):  # Limit to 15 for token efficiency
            scratchpad += f"\n{idx}. CVE: {cve_id}\n"
            scratchpad += f"   Package: {doc.get('package_name', 'N/A')}\n"
            scratchpad += f"   Ecosystem: {doc.get('ecosystem', 'N/A')}\n"
            scratchpad += f"   Severity: {doc.get('severity', 'N/A')}\n"
            scratchpad += f"   CVSS Score: {doc.get('cvss_score', 'N/A')}\n"
            scratchpad += f"   Vulnerability Type: {doc.get('vulnerability_type', 'N/A')}\n"
            
            if doc.get("description"):
                desc = doc['description']
                scratchpad += f"   Description: {desc[:300] if len(desc) > 300 else desc}\n"

            if doc.get("advisory_text"):
                advisory = doc['advisory_text']
                scratchpad += f"   Advisory: {advisory[:400] if len(advisory) > 400 else advisory}\n"
        
        if documents_collected > 15:
            scratchpad += f"\n... and {documents_collected - 15} more documents\n"
    
    # Build the ReAct format prompt
    prompt = f"""Use the following format:

Thought: you should always think about what to do
Action: the action to take, should be one of [search_vulnerabilities]
Action Input: the input parameters for the action
Observation: the result of the action
Thought: reflect on the observations
... (this Thought/Action/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: <COMPLETE COMPREHENSIVE ANSWER WITH ALL CITATIONS AND FORMATTING>

Question: {user_question}{scratchpad}

{'='*80}
CRITICAL DECISION POINT - Iteration {iteration}:

You have collected:
- {documents_collected} CVE documents
- {aggregations_collected} aggregation fields
- {len(previous_searches)} searches completed

{("üõë THIS IS YOUR FINAL ITERATION - YOU MUST PROVIDE 'Final Answer:' WITH A COMPLETE ANSWER NOW") if is_final_iteration else ""}

MANDATORY RULES:
1. If you have enough information to answer ‚Üí YOU MUST provide "Final Answer:"
2. If you have 1+ documents with advisory content ‚Üí YOU MUST provide "Final Answer:"
3. If you have aggregation data ‚Üí YOU MUST provide "Final Answer:"
4. DO NOT search again if you already have relevant data
5. DO NOT repeat a search you already performed (check search history above)
6. DO NOT respond with "Action:", "Thought:", "Action Input:" - ONLY "Final Answer:" or function call
{("‚ö†Ô∏è THIS IS YOUR FINAL ITERATION - YOU CANNOT SEARCH ANYMORE. PROVIDE FINAL ANSWER WITH YOUR BEST SYNTHESIS OF COLLECTED DATA.") if is_final_iteration else ""}

‚ö†Ô∏è WARNING: You already searched {len(previous_searches)} time(s). DO NOT repeat identical searches!
Review the search history above - if your next search would be identical or very similar, provide Final Answer instead.

DECISION CRITERIA:
‚úÖ Have aggregation data? ‚Üí "Final Answer: <complete answer>"
‚úÖ Have several documents? ‚Üí "Final Answer: <complete answer>"
‚úÖ Previous searches returned 0 results? ‚Üí "Final Answer: <explanation>"

‚ö†Ô∏è CRITICAL FORMAT REQUIREMENT:
Your response MUST be ONE of these two options:
  OPTION 1: A function call to search_vulnerabilities() - only if you genuinely need more data
  OPTION 2: "Final Answer: <YOUR COMPLETE ANSWER HERE>" - provide the full answer immediately

‚ùå DO NOT respond with anything else
‚ùå DO NOT include "Thought:", "Action:", or any other text
‚ùå DO NOT say "I'll provide the answer" or "Let me search" - just DO IT
‚ùå DO NOT write "Final Answer:" without the complete answer text following it

IF YOU PROVIDE FINAL ANSWER:
- Start with "Final Answer:" prefix (case-insensitive, but "Final Answer:" is preferred)
- Generate the COMPLETE, COMPREHENSIVE answer immediately after the prefix
- Include ALL elements: CVE IDs, CVSS scores, packages, ecosystems, versions, code examples, remediation steps
- Use markdown formatting (headers ##, bullets, code blocks)
- End with grounding statement: "Source: X CVE records from the vulnerability database"
- DO NOT respond with ReAct format ("Thought:", "Action:") - provide the FINAL ANSWER NOW

Next step (MUST be either function call OR "Final Answer: <complete answer>"):"""
    return prompt
