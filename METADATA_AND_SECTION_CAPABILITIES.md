# `_extract_metadata()` & `_categorize_section()`: What They Enable

## The Two Methods at a Glance

```python
# Method 1: Extract structured metadata from advisory headers
_extract_metadata(content: str) -> dict
# Returns: {"cve_id": "CVE-2024-1234", "package_name": "...", "ecosystem": "npm", ...}

# Method 2: Categorize advisory sections by semantic meaning
_categorize_section(title: str) -> str
# Returns: "summary" | "remediation" | "code_example" | "attack" | "cvss" | "details"
```

These two methods are the **bridge between unstructured advisory text and structured search**. They transform raw markdown into queryable data.

---

## ðŸ—ï¸ What They Do: The Big Picture

### Before These Methods
```
Advisory markdown file (raw, unstructured):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ # CVE-2024-1234: Express Validator      â”‚
â”‚ **CVE ID:** CVE-2024-1234               â”‚
â”‚ **Package:** express-validator          â”‚
â”‚ **Ecosystem:** npm                      â”‚
â”‚ **CVSS Score:** 9.8                     â”‚
â”‚                                         â”‚
â”‚ ## Summary                              â”‚
â”‚ This vulnerability allows...            â”‚
â”‚                                         â”‚
â”‚ ## Remediation                          â”‚
â”‚ Upgrade to version 1.0.2                â”‚
â”‚                                         â”‚
â”‚ ## Code Examples                        â”‚
â”‚ ```js                                   â”‚
â”‚ const validator = require(...)          â”‚
â”‚ ```                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Problem: All text, no structure. Can't filter by ecosystem, severity, or section type.
Can only do keyword search or semantic similarity.
```

### After These Methods
```
Structured Typesense document:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Top-level fields (extracted by _extract_metadata):      â”‚
â”‚ {                                                       â”‚
â”‚   "cve_id": "CVE-2024-1234",     â† Filter by CVE       â”‚
â”‚   "package_name": "express-validator", â† Filter by pkg  â”‚
â”‚   "ecosystem": "npm",            â† Filter by ecosystem   â”‚
â”‚   "severity": "Critical",        â† Filter by severity    â”‚
â”‚   "cvss_score": 9.8,             â† Range filter/aggregateâ”‚
â”‚                                                         â”‚
â”‚   Nested advisory chunks (categorized by section):      â”‚
â”‚   "advisory_chunks": [                                  â”‚
â”‚     {                                                   â”‚
â”‚       "content": "This vulnerability allows...",        â”‚
â”‚       "section": "summary"     â† Facet by section       â”‚
â”‚     },                                                  â”‚
â”‚     {                                                   â”‚
â”‚       "content": "Upgrade to version 1.0.2",           â”‚
â”‚       "section": "remediation" â† Facet by section       â”‚
â”‚     },                                                  â”‚
â”‚     {                                                   â”‚
â”‚       "content": "```js...",                           â”‚
â”‚       "section": "code_example", â† Facet by section     â”‚
â”‚       "is_code": true           â† Facet by code flag    â”‚
â”‚     }                                                   â”‚
â”‚   ]                                                     â”‚
â”‚ }                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Result: Structured queryable data. Can filter, facet, aggregate.
```

---

## ðŸŽ¯ Method 1: `_extract_metadata()` - Structured Filtering & Analytics

### What It Does

Parses advisory header lines to extract key metadata:

```python
def _extract_metadata(content: str) -> dict:
    """Extracts: cve_id, package_name, ecosystem, severity, cvss_score"""
    metadata = {}
    for line in content.split("\n")[:20]:  # Check first 20 lines
        if "**CVE ID:**" in line:
            metadata["cve_id"] = line.split("**CVE ID:**")[1].strip()
        elif "**Package:**" in line:
            metadata["package_name"] = ...
        elif "**Ecosystem:**" in line:
            metadata["ecosystem"] = ...
        elif "**Severity:**" in line:
            metadata["severity"] = ...
        elif "**CVSS Score:**" in line:
            metadata["cvss_score"] = float(...)
    return metadata
```

### Why This Matters

**Problem it solves:**
- Advisory files have metadata embedded in markdown text
- Can't search/filter without extracting it
- Metadata duplication: same info in CSV + advisory header = inconsistency

**Solution:**
- Extract once during ingestion
- Store as indexed fields in Typesense
- Enable structured queries: "Show npm vulnerabilities" (no need to parse advisory files)

### Questions It Allows You to Answer

#### 1ï¸âƒ£ **Direct Filtering Queries**
```python
# "Show me all Critical npm vulnerabilities"
search_vulnerabilities(
    ecosystems=["npm"],
    severity_levels=["Critical"]
)

# Behind the scenes:
# Typesense filters on extracted "ecosystem" and "severity" fields
# Result: 8 CVEs instantly (no full-text parsing needed)
```

#### 2ï¸âƒ£ **Aggregation & Statistics Queries**
```python
# "What's the average CVSS score by ecosystem?"
search_vulnerabilities(
    query="*",
    facet_by="ecosystem,cvss_score"
)

# Results:
# {
#   "ecosystem": {"npm": 20, "pip": 15, "maven": 12},
#   "cvss_score": {
#     "npm_avg": 7.8,
#     "pip_avg": 7.2,
#     "maven_avg": 8.1
#   }
# }

# Can now generate: "npm has avg CVSS 7.8, pip 7.2, maven 8.1"
```

#### 3ï¸âƒ£ **Range Filtering Queries**
```python
# "Show me high-impact vulnerabilities (CVSS >= 8.0)"
search_vulnerabilities(
    min_cvss_score=8.0
)

# Behind the scenes:
# Typesense filters: cvss_score >= 8.0
# Result: 15 CVEs with critical impact instantly
```

#### 4ï¸âƒ£ **Multi-Dimensional Analysis**
```python
# "For each ecosystem, how many Critical vulnerabilities are there?"
results = {}
for eco in ["npm", "pip", "maven"]:
    result = search_vulnerabilities(
        ecosystems=[eco],
        severity_levels=["Critical"],
        facet_by="ecosystem"  # To confirm count
    )
    results[eco] = result.total_found

# Output:
# {"npm": 8, "pip": 5, "maven": 4}
# Insight: npm has 1.6x more critical vulnerabilities than pip
```

#### 5ï¸âƒ£ **Dashboard/Reporting Queries**
```python
# Generate security dashboard
dashboard = search_vulnerabilities(
    query="*",
    facet_by="severity,ecosystem,vulnerability_type"
)

# Single query returns:
# - 8 Critical, 15 High, 24 Medium
# - 20 npm, 15 pip, 12 maven
# - 6 XSS, 5 SQL Injection, 3 RCE
# No Python loops or post-processing needed!
```

#### 6ï¸âƒ£ **Time-Series Analytics**
```python
# "How many vulnerabilities per month?"
# (If published_date was extracted - it's in the code!)
search_vulnerabilities(
    query="*",
    facet_by="published_date"
)

# Can track vulnerability trends over time
```

### Impact: Without vs With `_extract_metadata()`

| Query | Without Extraction | With Extraction |
| --- | --- | --- |
| "Critical npm vulnerabilities?" | Parse every advisory file manually | Single filtered query |
| "Avg CVSS by ecosystem?" | Manual aggregation across 47 files | `facet_by="ecosystem,cvss_score"` |
| "CVSS >= 8.0?" | Regex parsing in Python | `min_cvss_score=8.0` |
| Speed | Seconds (file I/O) | Milliseconds (indexed search) |
| Accuracy | Error-prone (manual parsing) | 100% (parsed once) |

---

## ðŸ“‚ Method 2: `_categorize_section()` - Advisory Structure Queries

### What It Does

Analyzes markdown section titles and assigns semantic categories:

```python
def _categorize_section(title: str) -> str:
    """Maps section titles to semantic categories"""
    title_lower = title.lower()
    
    if "summary" in title_lower or "overview" in title_lower:
        return "summary"
    elif "remediat" in title_lower or "fix" in title_lower:
        return "remediation"
    elif "attack" in title_lower or "exploit" in title_lower:
        return "attack"
    elif "code" in title_lower or "example" in title_lower:
        return "code_example"
    elif "cvss" in title_lower:
        return "cvss"
    return "details"
```

### Why This Matters

**Problem it solves:**
- Advisory sections vary widely: "## Remediation", "## Fix", "## Solution", "## Remedial Steps"
- Can't reliably search for "remediation" when titles are inconsistent
- No way to know which CVEs have code examples, attack vectors, etc.

**Solution:**
- Normalize section titles to semantic categories
- Store as indexed field for each advisory chunk
- Enable faceted search on advisory structure

### Questions It Allows You to Answer

#### 1ï¸âƒ£ **Content Completeness Queries**
```python
# "Which CVEs have remediation steps?"
search_vulnerabilities(
    additional_filters="advisory_chunks.section:remediation"
)

# Result: 8 CVEs with documented remediation
# Without categorization: Can't answer (would need keyword matching)
```

#### 2ï¸âƒ£ **Documentation Quality Metrics**
```python
# "What % of advisories have code examples?"
all_cves = search_vulnerabilities(query="*", facet_by="has_advisory")
# Result: 8 CVEs have advisories

code_coverage = search_vulnerabilities(
    additional_filters="advisory_chunks.section:code_example",
    facet_by="advisory_chunks.section"
)
# Result: 7 CVEs have code examples
# Metric: 7/8 = 87.5% code example coverage
```

#### 3ï¸âƒ£ **Section Distribution Analysis**
```python
# "What types of sections appear most in our advisories?"
search_vulnerabilities(
    query="*",
    facet_by="advisory_chunks.section"
)

# Results:
# {
#   "summary": 12,       # Most common
#   "details": 10,
#   "remediation": 8,
#   "code_example": 7,
#   "attack": 5,
#   "cvss": 3            # Least common
# }

# Insight: Advisories focus on summary/details, less on attack vectors
```

#### 4ï¸âƒ£ **Targeted Content Retrieval**
```python
# "Show me attack vector explanations for this vulnerability"
search_vulnerabilities(
    cve_ids=["CVE-2024-1234"],
    additional_filters="advisory_chunks.section:attack"
)

# Result: Only the "attack" sections from that CVE
# Without categorization: Would return entire advisory
```

#### 5ï¸âƒ£ **Code Coverage by Severity**
```python
# "Do critical vulnerabilities have code examples?"
search_vulnerabilities(
    severity_levels=["Critical"],
    additional_filters="advisory_chunks.section:code_example",
    facet_by="severity"
)

# Result: How many critical CVEs have code examples?
# Can compare: do high-severity CVEs tend to have more code examples?
```

#### 6ï¸âƒ£ **Documentation Completeness Scorecard**
```python
# Generate documentation quality metrics
metrics = {}

for section_type in ["summary", "remediation", "code_example", "attack"]:
    result = search_vulnerabilities(
        additional_filters=f"advisory_chunks.section:{section_type}",
        facet_by="advisory_chunks.section"
    )
    metrics[section_type] = result.total_found

# Output:
# {
#   "summary": 12,
#   "remediation": 8,
#   "code_example": 7,
#   "attack": 5
# }
# 
# Interpretation: 
# - 87.5% have summaries (12/8 advisories)
# - 75% have remediation (8/8 advisories... wait, doesn't add up)
# - Better to count unique CVEs per section type
```

#### 7ï¸âƒ£ **Combined CSV + Advisory Queries**
```python
# "Show critical npm vulnerabilities with code examples"
search_vulnerabilities(
    ecosystems=["npm"],
    severity_levels=["Critical"],
    additional_filters="advisory_chunks.section:code_example",
    facet_by="advisory_chunks.section"
)

# Result:
# {
#   "documents": [CVEs matching all filters],
#   "total_found": 3,
#   "aggregations": {
#     "advisory_chunks.section": {
#       "code_example": 3
#     }
#   }
# }

# Answer: "3 critical npm vulnerabilities have code examples"
```

### Impact: Without vs With `_categorize_section()`

| Query | Without Categorization | With Categorization |
| --- | --- | --- |
| "CVEs with remediation?" | Keyword search for "fix/patch/remediate" (misses variants) | `advisory_chunks.section:remediation` (exact) |
| "Code example coverage?" | Manual parsing each advisory | `additional_filters="advisory_chunks.section:code_example"` |
| "Which sections most common?" | Manual count across all advisories | `facet_by="advisory_chunks.section"` |
| "Critical + code examples?" | Multiple keyword searches, manual merge | Single filtered query |
| Speed | Seconds (multiple searches) | Milliseconds (single query) |
| Accuracy | Error-prone (keyword matching) | 100% (semantic categorization) |

---

## ðŸ”— Combined Power: Both Methods Together

### Real-World Scenario
```
Question: "I need to prioritize security training. Which npm team 
should I train first - the one working on critical vulnerabilities 
or the one with high CVSS but lower severity ratings?"

Without these methods:
â”œâ”€ Parse all advisory files for ecosystem: npm
â”œâ”€ Manually extract CVSS scores
â”œâ”€ Categorize severity levels
â”œâ”€ Count and aggregate
â””â”€ Takes minutes, error-prone

With these methods:
search_vulnerabilities(
    ecosystems=["npm"],
    severity_levels=["Critical"],
    facet_by="cvss_score,vulnerability_type"
)
# Result in milliseconds!
```

### Another Real-World Scenario
```
Question: "We're releasing a new security tool. Which vulnerability 
types should we prioritize? Show me ones that are well-documented 
with code examples and clear remediation steps."

Without these methods:
â”œâ”€ Search for "code example" keyword
â”œâ”€ Search for "remediation" keyword
â”œâ”€ Manually check which are vulnerability types
â”œâ”€ Merge results
â””â”€ Repeated searches, slow

With these methods:
search_vulnerabilities(
    additional_filters="advisory_chunks.section:code_example && advisory_chunks.section:remediation",
    facet_by="vulnerability_type",
    facet_by="has_advisory"
)

# Results:
# {
#   "vulnerability_type": {"XSS": 3, "SQL Injection": 2, "RCE": 1},
#   "has_advisory": {"true": 6}
# }
# 
# Answer: "XSS is most documented with code + remediation (3 CVEs)"
```

---

## ðŸ“Š Types of Questions They Answer

### Category 1: Existence Questions (Binary)
```
"Does CVE-2024-1234 have a remediation section?"
search_vulnerabilities(
    cve_ids=["CVE-2024-1234"],
    additional_filters="advisory_chunks.section:remediation"
)
# Result: Found or not found
```

### Category 2: Counting Questions (Aggregation)
```
"How many critical vulnerabilities are there?"
search_vulnerabilities(
    severity_levels=["Critical"],
    facet_by="severity"
)
# Result: Count of critical CVEs
```

### Category 3: Distribution Questions (Faceting)
```
"Show vulnerability distribution by severity"
search_vulnerabilities(
    query="*",
    facet_by="severity"
)
# Result: Critical: 8, High: 15, Medium: 24, Low: 0
```

### Category 4: Statistics Questions (Aggregation)
```
"What's the average CVSS score?"
search_vulnerabilities(
    query="*",
    facet_by="cvss_score"
)
# Result: min: 4.1, max: 9.8, avg: 7.2
```

### Category 5: Filtering + Analytics Questions (Combined)
```
"Average CVSS for each ecosystem?"
for eco in ["npm", "pip", "maven"]:
    result = search_vulnerabilities(
        ecosystems=[eco],
        facet_by="cvss_score"
    )
    print(f"{eco}: avg={result.aggregations['cvss_score']['avg']}")
```

### Category 6: Content Structure Questions (Section-Aware)
```
"What % of npm CVEs have code examples?"
npm_total = search_vulnerabilities(ecosystems=["npm"]).total_found
npm_with_code = search_vulnerabilities(
    ecosystems=["npm"],
    additional_filters="advisory_chunks.section:code_example"
).total_found
percentage = npm_with_code / npm_total * 100
```

### Category 7: Complex Multi-Dimensional Questions
```
"For each severity level, show the percentage of CVEs with 
remediation steps, grouped by ecosystem"

for severity in ["Critical", "High", "Medium"]:
    for ecosystem in ["npm", "pip", "maven"]:
        total = search_vulnerabilities(
            severity_levels=[severity],
            ecosystems=[ecosystem]
        ).total_found
        
        with_remediation = search_vulnerabilities(
            severity_levels=[severity],
            ecosystems=[ecosystem],
            additional_filters="advisory_chunks.section:remediation"
        ).total_found
        
        percentage = with_remediation / total * 100
        print(f"{severity} {ecosystem}: {percentage}% have remediation")
```

---

## ðŸŽ“ For Your Interview

### If Asked: "What's the value of `_extract_metadata()`?"

*A:* "It transforms unstructured advisory headers into queryable fields. By extracting CVE ID, package name, ecosystem, severity, and CVSS score, we can:

1. **Enable structured filtering**: Users can filter by 'npm + critical + CVSS >= 8.0' instead of keyword searching
2. **Enable aggregations**: Get statistics like 'average CVSS by ecosystem' in a single query
3. **Enable faceted search**: Show distributions like '20 npm, 15 pip, 12 maven'
4. **Avoid inconsistency**: Parse once during ingestion, not multiple times per query
5. **Performance**: Indexed fields searched in milliseconds vs parsing files on every query

Without it, all queries would require full-text parsing of advisory files."

### If Asked: "What's the value of `_categorize_section()`?"

*A:* "It normalizes advisory sections to semantic categories. Advisories might say '## Remediation', '## Fix', '## Solution', or '## How to Patch' - all meaning the same thing. By categorizing these to 'remediation', we can:

1. **Enable section-specific search**: Find 'remediation' sections regardless of exact title wording
2. **Enable faceting on advisory structure**: Answer 'how many CVEs have code examples?'
3. **Enable content completeness metrics**: Measure documentation quality (do X% of critical vulnerabilities have remediation steps?)
4. **Enable targeted retrieval**: Show only the remediation section for a specific CVE, not the entire advisory

Without it, searching for remediation requires brittle keyword matching that misses title variations."

### If Asked: "Can't you just do keyword search on advisory text?"

*A:* "Yes, but with limitations:

- **Keyword search:** Searches 'remediation' text, finds any mention of 'fix' or 'patch' (imprecise)
- **Semantic categorization:** Only returns sections explicitly categorized as 'remediation' (precise)

Example:
- Advisory mentions: 'This vulnerability has no remediation available' (keyword search finds it, but it's NOT a remediation section)
- Categorized as: 'details' section (correct)

For analytics like 'coverage of remediations', keyword search over-counts. Categorization is exact."

---

## ðŸ’¾ Summary: What These Methods Enable

| Capability | Enabled By | Example Query |
| --- | --- | --- |
| **Filter by ecosystem** | `_extract_metadata()` | "Show npm vulnerabilities" |
| **Filter by severity** | `_extract_metadata()` | "Critical vulnerabilities only" |
| **Filter by CVSS range** | `_extract_metadata()` | "High-impact (CVSS >= 8.0)" |
| **Aggregate CVSS stats** | `_extract_metadata()` | "Average CVSS per ecosystem" |
| **Filter by section type** | `_categorize_section()` | "CVEs with remediation steps" |
| **Facet by section** | `_categorize_section()` | "Section distribution breakdown" |
| **Content completeness** | `_categorize_section()` | "% of advisories with code examples" |
| **Combined (CSV + advisory)** | **Both** | "Critical npm with remediation + code" |
| **Dashboard/reporting** | **Both** | Single query returning all metrics |

---

## ðŸš€ Key Insight

These two methods are the **bridge between unstructured text (advisories) and structured data (Typesense)**. They transform:

```
Raw markdown file â†’ Indexed searchable document
  Header text      â†’ Structured metadata (filter + aggregate)
  Section titles   â†’ Semantic categories (facet + retrieve)
  Section content  â†’ Embedded chunks (semantic search)
```

This is why they're mentioned in the ingest.py docstrings as "critical advantages of hybrid search engines over pure vector databases." Vector databases can only do semantic similarity. Search engines let you do filtering, faceting, aggregation, and rankingâ€”all powered by these extraction/categorization methods.
